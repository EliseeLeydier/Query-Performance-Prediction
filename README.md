
# √âvaluation de la qualit√© des requ√™tes avec QPP et LLM

Ce script √©value la qualit√© de requ√™tes de recherche en utilisant √† la fois des mesures QPP (pr√©-retrieval) telles que `idf`, `scq`, `ictf`, et un score qualitatif g√©n√©r√© par un LLM local(Ollama). Il compare ensuite ces scores √† la performance r√©elle (via `nDCG@10`) √† l‚Äôaide de `pytrec_eval`.

---

## üì¶ Pr√©requis

- Python 3.8+
- Java (pour Pyserini)
- Serveur LLM local compatible avec l‚ÄôAPI Ollama (port `11434`)
- Index Lucene pr√©construit pour le corpus (par ex. `robust04`)

### üìö Installation des d√©pendances

```bash
python3.10 -m venv env
source env/bin/activate

pip3 install pandas requests tqdm scipy pyserini pytrec_eval
```

---

## ‚öôÔ∏è Usage

### üîß Lancer le script :

```bash
python evaluate_queries.py --n 100
```

> `--n` : nombre de requ√™tes √† √©valuer (d√©faut : 250)

---

## üìÇ Fonctionnalit√©s principales

### √âtape 1 : Extraction des requ√™tes  
√Ä partir du corpus (ex: `robust04`), on r√©cup√®re les requ√™tes manuelles (`title`).

### √âtape 2 : Calcul des scores QPP  
- **idf** : Inverse Document Frequency  
- **scq** : Simplified Clarity Score  
- **ictf** : Inverse Collection Term Frequency

### √âtape 3 : Score qualitatif par LLM  
Envoi de chaque requ√™te au mod√®le local pour retour d‚Äôun score entre `0.00` et `1.00`.

### √âtape 4 : Corr√©lation avec performance r√©elle  
On compare les scores avec les `nDCG@10` obtenus via `pytrec_eval`.

---

## üìÅ R√©sultats

Les r√©sultats sont automatiquement sauvegard√©s dans un dossier :

```
resultPreRetrieval/YYYY-MM-DD_HH-MM-SS/
‚îú‚îÄ‚îÄ query_scores.json     # Scores QPP et LLM par requ√™te
‚îú‚îÄ‚îÄ notes.txt             # Corr√©lations et statistiques descriptives
```

---

## üìä Sorties principales

- **Matrice de corr√©lation** entre les scores (idf, scq, ictf, llm_score) et nDCG@10
- **Statistiques par requ√™te** : ID, scores QPP, score LLM, ndcg@10
- **Corr√©lations Kendall & Pearson** pour chaque score

---

## Exemple de sortie JSON

```json
[
  {
    "id": "303",
    "query": "Hubble telescope achievements",
    "idf": 3.14,
    "scq": 5.21,
    "ictf": 2.67,
    "llm_score": 0.81
  },
  ...
]
```

---

## üß™ Tests

> Pas de tests unitaires inclus, mais l‚Äôex√©cution du script avec `--n 5` permet un test rapide sur petit √©chantillon.

---

## üìù Notes

- Certaines requ√™tes (ex: ID 672) peuvent √™tre exclues du traitement.
- Le LLM doit r√©pondre **strictement** par un score flottant entre 0.00 et 1.00.


-------------

# √âvaluation de la Qualit√© des Requ√™tes avec Pyserini, QPP et un LLM

- de **scores QPP pr√©-retrieval** (`idf`, `scq`, `ictf`),
- d‚Äôun **score qualitatif produit par un LLM local** (via Ollama),
- et de la **performance r√©elle** (via `nDCG@10` √† partir des `qrels` et d‚Äôun `run` BM25).

---
## R√©sum√© des formules

# R√©sum√© des Formules QPP et nDCG

## 1. **IDF (Inverse Document Frequency)**

Mesure la raret√© d‚Äôun terme dans la collection :
$\[
\text{idf}(t) = \log \left( \frac{N}{df_t} \right)
\]$$

- *N* : nombre total de documents dans l‚Äôindex  
- *df‚Çú* : nombre de documents contenant le terme *t*

## 2. **SCQ (Similarity Collection Query)**

Mesure combin√©e de la fr√©quence du terme et de sa raret√© :

$\[
\text{scq}(t) = \left(1 + \log(1 + cf_t)\right) \cdot \log\left(1 + \frac{N}{df_t}\right)
\]$$

- *cf‚Çú* : fr√©quence totale du terme *t* dans la collection  
- *df‚Çú* : nombre de documents contenant le terme *t*

## 3. **ICTF (Inverse Collection Term Frequency)**

Une autre mesure de raret√© du terme dans toute la collection :

$\[
\text{ictf}(t) = \log\left( \frac{N}{cf_t} \right)
\]$$

## 4. **BM25 (Best Matching 25)**


```math
\text{BM25}(t, d) = \text{idf}(t) \cdot \frac{f(t, d) \cdot (k_1 + 1)}{f(t, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{\text{avgdl}})}
```


- \( t \) : terme de la requ√™te  
- \( d \) : document  
- \( f(t, d) \) : fr√©quence du terme \( t \) dans le document \( d \)  
- \( |d| \) : longueur du document (nombre total de mots)  
- \( \text{avgdl} \) : longueur moyenne des documents dans la collection  
- \( k_1 \) : param√®tre d'ajustement (souvent entre 1.2 et 2.0)  
- \( b \) : param√®tre de normalisation de la longueur (souvent 0.75)  
- \( \text{idf}(t) \) : inverse document frequency du terme \( t \)  

Formule de l‚ÄôIDF dans BM25 :

```math
\text{idf}(t) = \log \left( \frac{N - df_t + 0.5}{df_t + 0.5} + 1 \right)
```

- \( N \) : nombre total de documents  
- \( df_t \) : nombre de documents contenant le terme \( t \)


## 5. **nDCG@10 (Normalized Discounted Cumulative Gain)**

√âvalue la pertinence des r√©sultats en tenant compte de leur rang :

$\[
\text{nDCG@10} = \frac{DCG@10}{IDCG@10}
\]$$

o√π :

$\[
DCG@10 = \sum_{i=1}^{10} \frac{rel_i}{\log_2(i + 1)}
\]$$

$\[
IDCG@10 = \text{DCG@10 pour un classement id√©al}
\]$$

## 6. **Corr√©lations**

Pour mesurer la corr√©lation entre un score QPP et la qualit√© r√©elle (nDCG), on utilise :

- **Kendall tau** : mesure de concordance d‚Äôordre
- **Pearson** : mesure de corr√©lation lin√©aire

---

## G√©n√©ration du run BM25 (√† faire dans le terminal)

```bash
python -m pyserini.search.lucene --topics robust04 --index robust04 --output run.robust04.txt --bm25
```
